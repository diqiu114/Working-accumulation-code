内存对齐原因

以下为32位cpu

首先cpu理论上支持在任意地址读取数据，但是有的cpu不支持，原因是：

### 篇目1

理论上可以这么读，但不经济，原因在于CPU并不直接和内存打交道，因为内存实在太慢...了。
CPU 通常是和1级2级3级缓存打交道，不得已才读写内存。读写内存通常是一整块一整块的读，这样效率高。这就像汽车的油箱和加油站的关系，加油费时间，但一次加一箱和加一升的时间差别不太大。发动机会首先从油箱里取，没有了才会去加油站，没人一次加一升。

**那为什么一定要内存对齐呢？**

简单地说，还是因为这么做效率高。磁盘存储是分块的，内存也是按页管理，CPU 也是按32/64/128位来寻址和处理数据。如果可以任意偏移读写，会有的大量的计算都要浪费在偏移处理上，硬件设计上也不容易。所以没人这么做。

这就好比做工业化的鸡蛋篓，一篓装32个，可以设计好机器可以很愉快地自动装载，但你这一篓非要装31个，为实现单拿出一个的需求，需要设计复杂得多的机器/算法/操作才能实现。提这样需求的人会被打的，所以没有人这么干。

[本文链接](https://segmentfault.com/u/myskies)

### 篇目2：对篇目一偏移地址的解释

# CPU访问非对齐的内存时为何需要多次读取再拼接

![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/reprint.png)

[u010536615](https://blog.csdn.net/u010536615)![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/newCurrentTime2.png)于 2019-08-07 14:57:19 发布![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/articleReadEyes2.png)2388![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/tobarCollect2.png) 收藏 45

分类专栏： [技术 Cocos2dx c++](https://blog.csdn.net/u010536615/category_2815873.html) 文章标签： [内存](https://so.csdn.net/so/search/s.do?q=内存&t=all&o=vip&s=&l=&f=&viparticle=) [对齐](https://so.csdn.net/so/search/s.do?q=对齐&t=all&o=vip&s=&l=&f=&viparticle=) [非](https://so.csdn.net/so/search/s.do?q=非&t=all&o=vip&s=&l=&f=&viparticle=) [读取](https://so.csdn.net/so/search/s.do?q=读取&t=all&o=vip&s=&l=&f=&viparticle=) [多次](https://so.csdn.net/so/search/s.do?q=多次&t=all&o=vip&s=&l=&f=&viparticle=)

版权

[![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/resize,m_fixed,h_64,w_64.png)技术 Cocos2dx c++专栏收录该内容](https://blog.csdn.net/u010536615/category_2815873.html)

130 篇文章0 订阅

订阅专栏

原文地址：https://yangwang.hk/?p=773

最近在研究CPU的微内核，遇到了一个问题：**为何CPU要求内存访问对齐？**

换句话说：**CPU访问非对齐的内存时为何需要多次读取再拼接**？

首先简单说一下何为[内存](https://so.csdn.net/so/search?q=内存&spm=1001.2101.3001.7020)对齐。

例如，当cpu需要取4个连续的字节时，若内存起始位置的地址可以被4整除，那么我们称其对齐访问。

反之，则为未对齐访问。比如从地址0xf1取4字节就是非对齐（地址）访问。

简单的看来，对于一个数据总线宽度为32位的cpu，它一次拥有取出四字节数据的能力，理论上cpu应该是可以从任意的内存地址取四个连续字节的，而且是否对齐硬件的设计是相同的（如果内存和CPU都是字节组织的话，那么内存应当可以返回任意地址开始连续的四字节，CPU处理起来也没有任何差异）。

然而，很多cpu并不支持非对齐的内存访问，甚至在访问的时候会发生例外（例如arm架构的某些CPU）！而某些复杂指令集的cpu（比如x86架构），可以完成非对齐的内存访问，然而CPU也不是一次性读出四个字节，而是采取**多次读取对齐的内存**，然后进行数据拼接，从而实现非对齐数据访问的。如下图：

![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MzY2MTU=,size_16,color_FFFFFF,t_70.jpeg)

如果我们的数据存于内存的2-5中，在读取时实际上是先读取0-3，再读取4-7字节，再分别将2-3字节和4-5字节合并，最后得到所需的四字节数据。

那么为什么CPU不直接读取2-5，而是要么不提供支持，要么甚至不惜花大力气执行多次访问再拼接访问非对齐的内存呢（如此访问一则增加访问时间，二则增加电路的复杂性）？这背后一定有它的原因！

经过一番互联网搜索，但是在国内只能找到为什么写程序的时候要对齐的解释（因为CPU要么不支持，要么访问效率下降），然后是如何实现对齐。没有一篇文章从硬件原理上去分析为何访问非对齐内存如此麻烦。

最后我在神奇的StackOverflow网站上找到了相关的问题，以及合理的解答（看来并不是只有我一个人有类似的疑问）。

------

实际上，访问非对齐内存并没有我们想象的那么“简单”，例如，在一个常见的pc上，内存实际上是有多个内存芯片共同组成的（也就是内存条上那些黑色的内存颗粒）

为了提高访存的带宽，通常的做法是将地址分开，放到不同的芯片上，比如，第0-7bit在芯片0上储存，8-15bit在芯片2上组成，以此类推，如下图：

![img](%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA1MzY2MTU=,size_16,color_FFFFFF,t_70-16910553897685.jpeg)

这意味内存实际上并不是完全以byte形式组织的，而是以偏移量（offset）来给出具体地址的。

这样当我们采用对齐的地址访问时，比如从0x00开始访问四字节，显然四个字节储存于4个芯片，而且他们都有同样的偏移量(offset)，这时我们就能一次获得所需的数据。

但是当从0x01开始读取4字节呢？此时前三个字节也是按顺序分别储存在1-3芯片中的，而且偏移量都是0，但是第四个字节却储存在偏移量为1的芯片0中。

在访问内存时，CPU需要给出偏移量offset，而发送偏移量的总线宽度大约是40位（64bit环境下），通常这样的总线只有一个。

这意味着在一次内存访问周期内我们只能读取一个结果。

当然，要想一次读取两个offset的内容也不是不能实现，你可以增加用于发送地址的bus数量。对于一个64位的cpu，如果你希望在一个访问周期内读取未对齐的内存，你需要增加到8根总线。这意味着需要增加接近300个io。而通常cpu的管脚数量在700-2000之间，在这基础之上增加300将会是一个很大的改动。换句话说，就是会大大增加硬件的复杂程度。

同时，内存访问信号的频率是非常高的，增加的总线也会造成额外的噪声干扰。

当然，还有一种方法。由于非对齐访问最多也就访问两个不同的offset，而且这两个offset总是连续，我们可以再给内存内部加一根额外的线，这样就可以同时返回offset和offset+1两个偏移量上的数据了。

但是，这样意味着芯片内多了一些额外的加法器（用于给offset加一，得到下一个偏移量），所有的读操作都会在读取前增加一个计算操作。

这一步会降低内存的时钟。于是乎，我们可能为了千分之一概率出现的非对齐访问，增加了99.9%的对齐访问的访问延时。显然这并不是一个明智的选择。

因此，CPU不支持，或者通过两次读取来实现非对齐访问也就有理有据了。

当然，访问非对齐的数据还存在一个问题：cache

通常来说，cache是和offset相关联的，不同的offset被不同的cache line缓存，因此，访问非对齐的数据也意味着多次的cache读取，同样会降低效率。

综上所述，这些也基本上是访问非对齐内存需要多次读取的原因了。



## 关于C语言中结构体对齐方式

**对于结构体**

在结构体中，编译器为结构体的每个成员按其自然边界（alignment）分配空间。各个成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构体的地址相同。具体规则如下：

> - 1.第一个成员在结构体变量偏移量为0 的地址处，也就是第一个成员必须从头开始。
> - 2.以后每个成员相对于结构体首地址的 offset 都是该成员大小的整数倍，如有需要编译器会在成员之间加上填充字节。
> - 3.结构体的总大小为 最大对齐数的整数倍（每个成员变量都有自己的对齐数），如有需要编译器会在最末一个成员之后加上填充字节。
> - 4.如果嵌套结构体，嵌套的结构体对齐到自己的最大对齐数的整数倍处，结构体的整体大小就是所有最大对齐数（包含嵌套结构体的对齐数）的整数倍。



作者：卖馍工程师
链接：https://juejin.cn/post/6870162226032934926
来源：稀土掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

**上述第3点补充：**

贴上原文地址：http://stackoverflow.com/questions/10309089/why-does-size-of-the-struct-need-to-be-a-multiple-of-the-largest-alignment-of-an


以下是看完帖子后的个人理解，若是不对还请各位指出。

个人理解：第二条实际上还是为了内存对齐，如果没有第二条来善后，那第一条的工作就有可能白做了。举个例子

struct st{
    int32_t a;
    int8_t  b;
};

struct st arr[N];
结构体 st 如果在最后没有进行填充则应该是5个字节，若做了填充则是8个字节。而底下数组arr的地址分配则会受到结构体 st 大小的影响。

假设数组的起始地址为0，那么

st 为5字节时，arr[0] 占用 0-4，arr[0].a的startpos为0，arr[0].b的startpos为4；arr[1] 占用 5-9，arr[1].a的startpos为5，arr[1].b的startpos为9。可以看到arr[1]的内部成员a并没有对齐（startpos不是其数据类型大小的整数倍），借用帖子里的话 arr[1].a 会cross lines。

st 为8字节时，arr[0] 占用 0-7，arr[0].a的startpos为0，arr[0].b的startpos为4；arr[1] 占用 8-15，arr[1].a的startpos为8，arr[1].b的startpos为12。依次类推数组里所有结构体成员及所有结构体的内部成员都会对齐。
因为基础数据类型的数据大小无非就是1 2 4 8 16字节，若结构体的总大小是最大基础成员大小的整数倍，那么也就一定是其他任一基础成员大小的整数倍，那么每个结构体的startpos就一定是其任一基础成员大小的整数倍 ，这样的话，两条规则结合在一起就保证了所有基础类型数据、非基础类型数据全部对齐。
————————————————
版权声明：本文为CSDN博主「路人1994」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/zyz770834013/article/details/71909055